{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65967224",
   "metadata": {},
   "source": [
    "functions needed for testing in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a5a8698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<setuptools.extension.Extension('sscAnnotat3D.app') at 0x7f6f1deb24c0>, <setuptools.extension.Extension('sscAnnotat3D.__version__') at 0x7f6f1de66940>, <setuptools.extension.Extension('sscAnnotat3D.aux_functions') at 0x7f6f1de66970>, <setuptools.extension.Extension('sscAnnotat3D.utils') at 0x7f6f1de73d60>, <setuptools.extension.Extension('sscAnnotat3D.__init__') at 0x7f6f1de73d90>, <setuptools.extension.Extension('sscAnnotat3D.superpixels') at 0x7f6f1de73dc0>, <setuptools.extension.Extension('sscAnnotat3D.label') at 0x7f6f1de73df0>, <setuptools.extension.Extension('sscAnnotat3D.binary') at 0x7f6f1de73e20>, <setuptools.extension.Extension('sscAnnotat3D.progressbar') at 0x7f6f1de73e50>]\n",
      "[]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.repository.data_repo') at 0x7f6f1de73cd0>, <setuptools.extension.Extension('sscAnnotat3D.repository.__init__') at 0x7f6f1de73eb0>, <setuptools.extension.Extension('sscAnnotat3D.repository.module_repo') at 0x7f6f1de73d30>]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.api.superpixel') at 0x7f6f1de73e80>, <setuptools.extension.Extension('sscAnnotat3D.api.io') at 0x7f6f1de736a0>, <setuptools.extension.Extension('sscAnnotat3D.api.deep') at 0x7f6f1de73730>, <setuptools.extension.Extension('sscAnnotat3D.api.__init__') at 0x7f6f1de73f10>, <setuptools.extension.Extension('sscAnnotat3D.api.remotevis') at 0x7f6f1de73ee0>, <setuptools.extension.Extension('sscAnnotat3D.api.annotation') at 0x7f6f1de73fa0>, <setuptools.extension.Extension('sscAnnotat3D.api.filters') at 0x7f6f1de73f40>, <setuptools.extension.Extension('sscAnnotat3D.api.image') at 0x7f6f1de73f70>]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.api.modules.superpixel_segmentation_module') at 0x7f6f1de73fd0>, <setuptools.extension.Extension('sscAnnotat3D.api.modules.pixel_segmentation_module') at 0x7f6f1dba5040>, <setuptools.extension.Extension('sscAnnotat3D.api.modules.__init__') at 0x7f6f1dba5070>]\n",
      "[]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.modules.annotation_module') at 0x7f6f1dba50a0>, <setuptools.extension.Extension('sscAnnotat3D.modules.classifier_segmentation_module') at 0x7f6f1dba5100>, <setuptools.extension.Extension('sscAnnotat3D.modules.deep_network_module') at 0x7f6f1dba50d0>, <setuptools.extension.Extension('sscAnnotat3D.modules.superpixel_segmentation_module') at 0x7f6f1dba5130>, <setuptools.extension.Extension('sscAnnotat3D.modules.segmentation_module') at 0x7f6f1dba5160>, <setuptools.extension.Extension('sscAnnotat3D.modules.pixel_segmentation_module') at 0x7f6f1dba5190>, <setuptools.extension.Extension('sscAnnotat3D.modules.__init__') at 0x7f6f1dba51c0>]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.deeplearning.deeplearning_workspace_dialog') at 0x7f6f1dba51f0>, <setuptools.extension.Extension('sscAnnotat3D.deeplearning.__init__') at 0x7f6f1dba5220>]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.cython.__init__') at 0x7f6f1dba5250>, <setuptools.extension.Extension('sscAnnotat3D.cython.annotation') at 0x7f6f1dba52b0>, <setuptools.extension.Extension('sscAnnotat3D.cython.annotation') at 0x7f6f1dba5280>]\n",
      "Compiling sscAnnotat3D/repository/data_repo.py because it changed.\n",
      "Compiling sscAnnotat3D/api/deep.py because it changed.\n",
      "[1/2] Cythonizing sscAnnotat3D/api/deep.py\n",
      "[2/2] Cythonizing sscAnnotat3D/repository/data_repo.py\n",
      "running install\n",
      "/home/brunocarlos_lnls/.conda/envs/webdev/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "running build\n",
      "running build_py\n",
      "running egg_info\n",
      "writing sscAnnotat3D.egg-info/PKG-INFO\n",
      "writing dependency_links to sscAnnotat3D.egg-info/dependency_links.txt\n",
      "writing requirements to sscAnnotat3D.egg-info/requires.txt\n",
      "writing top-level names to sscAnnotat3D.egg-info/top_level.txt\n",
      "reading manifest file 'sscAnnotat3D.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'sscAnnotat3D.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "building 'sscAnnotat3D.repository.data_repo' extension\n",
      "gcc -pthread -B /home/brunocarlos_lnls/.conda/envs/webdev/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/brunocarlos_lnls/.conda/envs/webdev/include -I/home/brunocarlos_lnls/.conda/envs/webdev/include -fPIC -O2 -isystem /home/brunocarlos_lnls/.conda/envs/webdev/include -fPIC -I/home/brunocarlos_lnls/.conda/envs/webdev/include/python3.9 -c sscAnnotat3D/repository/data_repo.c -o build/temp.linux-x86_64-cpython-39/sscAnnotat3D/repository/data_repo.o\n",
      "gcc -pthread -B /home/brunocarlos_lnls/.conda/envs/webdev/compiler_compat -shared -Wl,-rpath,/home/brunocarlos_lnls/.conda/envs/webdev/lib -Wl,-rpath-link,/home/brunocarlos_lnls/.conda/envs/webdev/lib -L/home/brunocarlos_lnls/.conda/envs/webdev/lib -L/home/brunocarlos_lnls/.conda/envs/webdev/lib -Wl,-rpath,/home/brunocarlos_lnls/.conda/envs/webdev/lib -Wl,-rpath-link,/home/brunocarlos_lnls/.conda/envs/webdev/lib -L/home/brunocarlos_lnls/.conda/envs/webdev/lib build/temp.linux-x86_64-cpython-39/sscAnnotat3D/repository/data_repo.o -o build/lib.linux-x86_64-cpython-39/sscAnnotat3D/repository/data_repo.cpython-39-x86_64-linux-gnu.so\n",
      "building 'sscAnnotat3D.api.deep' extension\n",
      "gcc -pthread -B /home/brunocarlos_lnls/.conda/envs/webdev/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/brunocarlos_lnls/.conda/envs/webdev/include -I/home/brunocarlos_lnls/.conda/envs/webdev/include -fPIC -O2 -isystem /home/brunocarlos_lnls/.conda/envs/webdev/include -fPIC -I/home/brunocarlos_lnls/.conda/envs/webdev/include/python3.9 -c sscAnnotat3D/api/deep.c -o build/temp.linux-x86_64-cpython-39/sscAnnotat3D/api/deep.o\n",
      "gcc -pthread -B /home/brunocarlos_lnls/.conda/envs/webdev/compiler_compat -shared -Wl,-rpath,/home/brunocarlos_lnls/.conda/envs/webdev/lib -Wl,-rpath-link,/home/brunocarlos_lnls/.conda/envs/webdev/lib -L/home/brunocarlos_lnls/.conda/envs/webdev/lib -L/home/brunocarlos_lnls/.conda/envs/webdev/lib -Wl,-rpath,/home/brunocarlos_lnls/.conda/envs/webdev/lib -Wl,-rpath-link,/home/brunocarlos_lnls/.conda/envs/webdev/lib -L/home/brunocarlos_lnls/.conda/envs/webdev/lib build/temp.linux-x86_64-cpython-39/sscAnnotat3D/api/deep.o -o build/lib.linux-x86_64-cpython-39/sscAnnotat3D/api/deep.cpython-39-x86_64-linux-gnu.so\n",
      "running install_lib\n",
      "copying build/lib.linux-x86_64-cpython-39/sscAnnotat3D/repository/data_repo.cpython-39-x86_64-linux-gnu.so -> /home/brunocarlos_lnls/.conda/envs/webdev/lib/python3.9/site-packages/sscAnnotat3D/repository\n",
      "copying build/lib.linux-x86_64-cpython-39/sscAnnotat3D/api/deep.cpython-39-x86_64-linux-gnu.so -> /home/brunocarlos_lnls/.conda/envs/webdev/lib/python3.9/site-packages/sscAnnotat3D/api\n",
      "running install_egg_info\n",
      "removing '/home/brunocarlos_lnls/.conda/envs/webdev/lib/python3.9/site-packages/sscAnnotat3D-1.3.0-py3.9.egg-info' (and everything under it)\n",
      "Copying sscAnnotat3D.egg-info to /home/brunocarlos_lnls/.conda/envs/webdev/lib/python3.9/site-packages/sscAnnotat3D-1.3.0-py3.9.egg-info\n",
      "running install_scripts\n",
      "    _                      _        _   _____ ____\n",
      "   / \\   _ __  _ __   ___ | |_ __ _| |_|___ /|  _ \\\n",
      "  / _ \\ | '_ \\| '_ \\ / _ \\| __/ _` | __| |_ \\| | | |\n",
      " / ___ \\| | | | | | | (_) | || (_| | |_ ___) | |_| |\n",
      "/_/   \\_\\_| |_|_| |_|\\___/ \\__\\__,_|\\__|____/|____/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd ../../ && python setup.py install && cd sscAnnotat3D/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10e8f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonify(obj):\n",
    "    # overwriting for tests using jupyter\n",
    "    return obj\n",
    "\n",
    "def handle_exception(msg):\n",
    "    print('error_msg: {} '.format(msg))\n",
    "\n",
    "class requestClass():\n",
    "    def __init__(self):\n",
    "        self.json = None\n",
    "\n",
    "    def set_json(self, newdict):\n",
    "        self.json = newdict\n",
    "        \n",
    "    def append_json(self, newdict):\n",
    "        self.json = {**self.json, **newdict}\n",
    "\n",
    "    def clear(self):\n",
    "        self.json = None\n",
    "\n",
    "global request \n",
    "request = requestClass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968c5f9",
   "metadata": {},
   "source": [
    "copying deep.py and deactivating flask functionalities and derived functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "797b05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This script contains some back-end functions for the deep learning module\n",
    "\n",
    "@authors : Gabriel Borin Macedo (gabriel.macedo@lnls.br or borinmacedo@gmail.com)\n",
    "         : Bruno Carlos (bruno.carlos@lnls.br)\n",
    "\n",
    "TODO : Don't forget to document the functions\n",
    "\n",
    "\"\"\"\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# from flask import Blueprint, jsonify, request\n",
    "# from flask_cors import cross_origin\n",
    "from werkzeug.exceptions import BadRequest\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sscIO.io import read_volume\n",
    "from sscDeepsirius.cython import standardize\n",
    "# from sscAnnotat3D.repository import data_repo\n",
    "from sscDeepsirius.utils import dataset, image, augmentation\n",
    "from sscDeepsirius.controller.inference_controller import InferenceController\n",
    "from sscDeepsirius.controller.host_network_controller import HostNetworkController \n",
    "from sscAnnotat3D.deeplearning import DeepLearningWorkspaceDialog\n",
    "\n",
    "\n",
    "# app = Blueprint('deep', __name__)\n",
    "\n",
    "def init_logger(init_msg : str = '\\nStarting message logger queue.\\n'):\n",
    "    data_repo.init_logger(init_msg)\n",
    "\n",
    "def log_msg(msg):\n",
    "    data_repo.set_log_message(msg)\n",
    "\n",
    "\n",
    "# @app.route(\"/read_log_queue\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def read_log_queue():\n",
    "    \"\"\"\n",
    "        Reads from a queue of messages stored on data_repo. \n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "            (str): An empty string if the queue is empty.\n",
    "    \"\"\"\n",
    "    msg = data_repo.dequeue_log_message()\n",
    "    if msg == None:\n",
    "        return ''\n",
    "    \n",
    "    return msg\n",
    "\n",
    "\n",
    "# @app.errorhandler(BadRequest)\n",
    "def handle_exception(error_msg: str):\n",
    "    \"\"\"\n",
    "    Function to handle error exception and returns to the user\n",
    "\n",
    "    Args:\n",
    "        error_msg (str): variable that contains the error\n",
    "\n",
    "    Returns:\n",
    "        (tuple): This function returns a tuple that contains the error as a JSON and an int 400\n",
    "\n",
    "    \"\"\"\n",
    "    return jsonify({\"error_msg\": error_msg}), 400\n",
    "\n",
    "\n",
    "# app.register_error_handler(400, handle_exception)\n",
    "\n",
    "\n",
    "def _convert_dtype_to_str(img_dtype: np.dtype):\n",
    "    \"\"\"\n",
    "    Build-in function to convert dtype to a str\n",
    "\n",
    "    Args:\n",
    "        img_dtype (np.dtype): np.dtype object that contains\n",
    "\n",
    "    Returns:\n",
    "        (str): returns the str version of the dtype\n",
    "\n",
    "    \"\"\"\n",
    "    return np.dtype(img_dtype).name\n",
    "\n",
    "\n",
    "def _debugger_print(msg: str, payload: any):\n",
    "    \"\"\"\n",
    "    Build-in function to user as debugger\n",
    "\n",
    "    Args:\n",
    "        msg(str): string message to user in debugger\n",
    "        payload(any): a generic payload\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"\\n----------------------------------------------------------\")\n",
    "    print(\"{} : {}\".format(msg, payload))\n",
    "    print(\"----------------------------------------------------------\\n\")\n",
    "\n",
    "# removed from io.py and placed here\n",
    "# @app.route(\"/open_new_workspace\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def open_new_workspace():\n",
    "    \"\"\"\n",
    "    Function that opens a new workspace\n",
    "\n",
    "    Notes:\n",
    "        the request.json[\"workspace_path\"] receives only the parameter \"selected_labels\"(str)\n",
    "\n",
    "    Returns:\n",
    "        (str): returns a string that contains the new workspace path\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workspace_path = request.json[\"workspace_path\"]\n",
    "        workspace_root = request.json[\"workspace_root\"]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    if (workspace_root == \"\"):\n",
    "        return handle_exception(\"Empty path isn't valid !\")\n",
    "\n",
    "    deep_model = DeepLearningWorkspaceDialog()\n",
    "    save_status, error_desc = deep_model.open_new_workspace(workspace_path)\n",
    "\n",
    "    if (save_status):\n",
    "        data_repo.set_deep_model_info(key='workspace_path', data=workspace_path)\n",
    "        return jsonify(workspace_path)\n",
    "\n",
    "    return handle_exception(\"unable to create the Workspace ! : {}\".format(error_desc))\n",
    "\n",
    "\n",
    "# @app.route(\"/load_workspace\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def load_workspace():\n",
    "    \"\"\"\n",
    "    Function that loads a created workspace\n",
    "\n",
    "    Notes:\n",
    "        the request.json[\"workspace_path\"] receives only the parameter \"selected_labels\"(str)\n",
    "\n",
    "    Returns:\n",
    "        (str): returns a string that contains the loaded workspace path\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workspace_path = request.json[\"workspace_path\"]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    deep_model = DeepLearningWorkspaceDialog()\n",
    "    check_valid_workspace = deep_model.check_workspace(workspace_path)\n",
    "\n",
    "    if (check_valid_workspace):\n",
    "        data_repo.set_deep_model_info(key='workspace_path', data=workspace_path)\n",
    "        return jsonify(check_valid_workspace)\n",
    "\n",
    "    return handle_exception(\"path \\\"{}\\\" is a invalid workspace path!\".format(workspace_path))\n",
    "\n",
    "\n",
    "def _augmenters_list(augmentation_vec: list = None, ion_range_vec: list = None):\n",
    "    \"\"\"\n",
    "    Build-in function that formats the front-end data into the correct structure in the function augment_web\n",
    "\n",
    "    Args:\n",
    "        augmentation_vec (list): a list that contains all the elements to augment\n",
    "        ion_range_vec (list): a list that contains the value of ion-rage of some augment parameters\n",
    "\n",
    "    Returns:\n",
    "        (list): returns a list that contains the params to augment and the respective values\n",
    "\n",
    "    \"\"\"\n",
    "    augmenters = []\n",
    "\n",
    "    # vertical-flip option\n",
    "    if (augmentation_vec[0][\"isChecked\"]):\n",
    "        augmenters.append(\"vertical-flip\")\n",
    "\n",
    "    # horizontal-flip option\n",
    "    if (augmentation_vec[1][\"isChecked\"]):\n",
    "        augmenters.append(\"horizontal-flip\")\n",
    "\n",
    "    # rotate-90-degrees option\n",
    "    if (augmentation_vec[2][\"isChecked\"]):\n",
    "        augmenters.append(\"rotate-90-degrees\")\n",
    "\n",
    "    # rotate-less-90-degrees (rotate 270 degrees) option\n",
    "    if (augmentation_vec[3][\"isChecked\"]):\n",
    "        augmenters.append(\"rotate-less-90-degrees\")\n",
    "\n",
    "    # contrast option\n",
    "    if (augmentation_vec[4][\"isChecked\"]):\n",
    "        # c_min, c_max = ion_range_vec[0][\"ionRangeLimit\"].values\n",
    "        c_lower, c_upper = ion_range_vec[0][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"contrast\",\n",
    "                 contrast=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in contrast\", augmentation_vec[-1])\n",
    "\n",
    "    # linear-contrast option\n",
    "    if (augmentation_vec[5][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[1][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"linear-contrast\",\n",
    "                 linearContrast=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in linear-contrast\", augmentation_vec[-1])\n",
    "\n",
    "    # dropout option\n",
    "    if (augmentation_vec[6][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[2][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"dropout\",\n",
    "                 dropout=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in dropout\", augmentation_vec[-1])\n",
    "\n",
    "    # gaussian-blur option\n",
    "    if (augmentation_vec[7][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[3][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"gaussian-blur\",\n",
    "                 sigma=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in gaussian-blur\", augmentation_vec[-1])\n",
    "\n",
    "    # average-blur option\n",
    "    if (augmentation_vec[8][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[4][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"average-blur\",\n",
    "                 k=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in average-blur\", augmentation_vec[-1])\n",
    "\n",
    "    # additive-poisson-noise option\n",
    "    if (augmentation_vec[9][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[5][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"additive-poisson-noise\",\n",
    "                 k=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in additive-poisson-noise\", augmentation_vec[-1])\n",
    "\n",
    "    # elastic-deformation option\n",
    "    if (augmentation_vec[10][\"isChecked\"]):\n",
    "        c_lower_alpha, c_upper_alpha = ion_range_vec[6][\"actualRangeVal\"].values()\n",
    "        c_lower_sigma, c_upper_sigma = ion_range_vec[7][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"elastic-deformation\",\n",
    "                 alpha=(c_lower_alpha, c_upper_alpha),\n",
    "                 sigma=(c_lower_sigma, c_upper_sigma)))\n",
    "        _debugger_print(\"test for ion-range in elastic-deformation\", augmentation_vec[-1])\n",
    "\n",
    "    _debugger_print(\"augmenters param\", augmenters)\n",
    "    return augmenters\n",
    "\n",
    "\n",
    "# @app.route(\"/create_dataset\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Function that creates the .h5 dataset\n",
    "\n",
    "    Notes:\n",
    "        This function is used in DatasetComp.tsx\n",
    "\n",
    "    Returns:\n",
    "        (dict): returns a dict that contains the dataset .h5 name. Otherwise, will return an error for the front-end\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        output = request.json[\"file_path\"]\n",
    "        sample = request.json[\"sample\"]\n",
    "        augmentation_vec = request.json[\"augmentation\"]\n",
    "        ion_range_vec = request.json[\"ion_range_vec\"]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    size = (sample[\"patchSize\"][0], sample[\"patchSize\"][1], sample[\"patchSize\"][2])\n",
    "    num_classes = sample[\"nClasses\"]\n",
    "    nsamples = sample[\"sampleSize\"]\n",
    "    offset = (0, 0, 0)\n",
    "    logging.debug('size = {}, nsamples = {}'.format(size, sample[\"sampleSize\"]))\n",
    "    augmenter_params = _augmenters_list(augmentation_vec, ion_range_vec)\n",
    "\n",
    "    imgs = list(data_repo.get_all_dataset_data().values())\n",
    "    labels = list(data_repo.get_all_dataset_label().values())\n",
    "    weights = list(data_repo.get_all_dataset_weight().values())\n",
    "\n",
    "    data, error_status = dataset.create_dataset_web(imgs, labels, weights,\n",
    "                                            output, nsamples, num_classes,\n",
    "                                            size, offset)\n",
    "\n",
    "    if (not data):\n",
    "        return handle_exception(error_status)\n",
    "\n",
    "    initial_output = output\n",
    "    splited_str = output.split(\"/\")\n",
    "    dataset_name = splited_str[-1]\n",
    "    new_dataset_name = splited_str[-1].split(\".\")[0] + \"_augment\" + \".h5\"\n",
    "    output = output.replace(dataset_name, new_dataset_name)\n",
    "\n",
    "    if (augmenter_params):\n",
    "        data, error_status = augmentation.augment_web(output, initial_output, augmenter_params, data)\n",
    "\n",
    "    if (not data):\n",
    "        return handle_exception(error_status)\n",
    "\n",
    "    dataset.save_dataset(data)\n",
    "\n",
    "    return jsonify({\"datasetFilename\": initial_output.split(\"/\")[-1]})\n",
    "\n",
    "\n",
    "# @app.route(\"/open_inference_files/<file_id>\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def open_inference_files(file_id: str):\n",
    "    _debugger_print(\"new key\", file_id)\n",
    "    try:\n",
    "        file_path = request.json[\"image_path\"]\n",
    "    except:\n",
    "        return handle_exception(\"Error while trying to get the image path\")\n",
    "\n",
    "    try:\n",
    "        file_dtype = request.json[\"image_dtype\"]\n",
    "    except:\n",
    "        return handle_exception(\"Error while trying to get the image dtype\")\n",
    "\n",
    "    file = file_path.split(\"/\")[-1]\n",
    "    file_name, extension = os.path.splitext(file)\n",
    "\n",
    "    if (file == \"\"):\n",
    "        return handle_exception(\"Empty path isn't valid !\")\n",
    "\n",
    "    raw_extensions = [\".raw\", \".b\"]\n",
    "    tif_extensions = [\".tif\", \".tiff\", \".npy\", \".cbf\"]\n",
    "\n",
    "    extensions = [*raw_extensions, *tif_extensions]\n",
    "\n",
    "    if extension not in extensions:\n",
    "        return handle_exception(\"The extension {} isn't supported !\".format(extension))\n",
    "\n",
    "    error_msg = \"\"\n",
    "\n",
    "    try:\n",
    "        use_image_raw_parse = request.json[\"use_image_raw_parse\"]\n",
    "        if (extension in tif_extensions or use_image_raw_parse):\n",
    "            start = time.process_time()\n",
    "            image, info = read_volume(file_path, 'numpy')\n",
    "            end = time.process_time()\n",
    "            error_msg = \"No such file or directory {}\".format(file_path)\n",
    "\n",
    "            if (_convert_dtype_to_str(image.dtype) != file_dtype and (file_id == \"image\" or file_id == \"label\")):\n",
    "                image = image.astype(file_dtype)\n",
    "\n",
    "        else:\n",
    "            image_raw_shape = request.json[\"image_raw_shape\"]\n",
    "            start = time.process_time()\n",
    "            image, info = read_volume(file_path, 'numpy',\n",
    "                                               shape=(image_raw_shape[2], image_raw_shape[1], image_raw_shape[0]),\n",
    "                                               dtype=file_dtype)\n",
    "            end = time.process_time()\n",
    "            error_msg = \"Unable to reshape the volume {} into shape {} and type {}. \" \\\n",
    "                        \"Please change the dtype and shape and load the image again\".format(file, request.json[\n",
    "                \"image_raw_shape\"], file_dtype)\n",
    "        image_shape = image.shape\n",
    "        image_dtype = _convert_dtype_to_str(image.dtype)\n",
    "    except:\n",
    "        return handle_exception(error_msg)\n",
    "\n",
    "    image_info = {\"fileName\": file_name + extension,\n",
    "                  \"shape\": image_shape,\n",
    "                  \"type\": image_dtype,\n",
    "                  \"scan\": info,\n",
    "                  \"time\": np.round(end - start, 2),\n",
    "                  \"size\": np.round(image.nbytes / 1000000, 2),\n",
    "                  \"filePath\": file_path}\n",
    "\n",
    "    data_repo.set_inference_data(key=file_id, data=image)\n",
    "    data_repo.set_inference_info(key=file_id, data=image_info)\n",
    "\n",
    "    return jsonify(image_info)\n",
    "\n",
    "\n",
    "# @app.route(\"/close_inference_file/<file_id>\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def close_inference_file(file_id: str):\n",
    "    \"\"\"\n",
    "    Function that deletes a file in inference a menu using a key as string\n",
    "\n",
    "    Args:\n",
    "        file_id (str): string used as key to delete the file\n",
    "\n",
    "    Returns:\n",
    "        (str): Returns a string that contains the error or \"success on delete the key i in Input Image inference\"\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_repo.del_inference_data(file_id)\n",
    "        data_repo.del_inference_info(file_id)\n",
    "        return jsonify(\"success on delete the key {} in Input Image inference\".format(file_id))\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "\n",
    "# @app.route(\"/close_all_files_dataset\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def close_all_inference_files():\n",
    "    \"\"\"\n",
    "    Function that delete all the files in inference menu\n",
    "\n",
    "    Returns:\n",
    "        (str): Returns a string that contains the error or \"Success to delete all data\"\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_repo.del_all_inference_data()\n",
    "        data_repo.del_all_inference_info()\n",
    "        return jsonify(\"Success to delete all data\")\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "\n",
    "\n",
    "# @app.route(\"/get_available_gpus\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "    Function that verify all the available gpus for inference and show to the user\n",
    "\n",
    "    Returns:\n",
    "        (dict): returns a dict that contains all the gpus to use for inference\n",
    "\n",
    "    \"\"\"\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    list_devices = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "    gpus = []\n",
    "    gpu_device_names = []\n",
    "    i = 0\n",
    "    for device in list_devices:\n",
    "        gpu_number = int(device.split(\":\")[-1])\n",
    "        gpus.append(gpu_number)\n",
    "        gpu_device_names.append({\n",
    "            \"key\": i,\n",
    "            \"value\": \"GPU {}\".format(gpu_number),\n",
    "            \"label\": device\n",
    "        })\n",
    "        i+=1\n",
    "\n",
    "    data_repo.set_inference_gpus(gpus)\n",
    "    return jsonify(gpu_device_names) \n",
    "\n",
    "\n",
    "# @app.route(\"/get_frozen_data\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def get_frozen_data():\n",
    "    \"\"\"\n",
    "    Function that verify all the frozen data in frozen directory created in the workspace menu\n",
    "\n",
    "    Returns:\n",
    "        (dict): returns a dict with all the meta_files for the user to choose and use in inference\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workspace_path = data_repo.get_deep_model_info('workspace_path')\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    try:\n",
    "        frozen_path_pb = glob.glob(workspace_path + \"frozen/*.pb\")\n",
    "        frozen_path_PB = glob.glob(workspace_path + \"frozen/*.PB\")\n",
    "        frozen_path = [*frozen_path_pb, *frozen_path_PB]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    meta_files = []\n",
    "\n",
    "    for i in range(0, len(frozen_path)):\n",
    "        file_name = frozen_path[i].split(\"/\")[-1]\n",
    "        meta_files.append({\n",
    "            \"key\": i,\n",
    "            \"value\": file_name,\n",
    "            \"label\": file_name\n",
    "        })\n",
    "\n",
    "    return jsonify(meta_files)\n",
    "\n",
    "\n",
    "# @app.route(\"/run_inference\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def run_inference():\n",
    "    \"\"\"\n",
    "    Function that run the inference\n",
    "\n",
    "    Returns:\n",
    "        (str): returns a string \"successes\" if the operation occurs without any error and an exception otherwise\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = request.json[\"output\"]\n",
    "        patches = request.json[\"patches\"]\n",
    "        network = request.json[\"network\"]\n",
    "        isInferenceOpChecked = request.json[\"isInferenceOpChecked\"]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    if (output[\"outputPath\"] == \"\"):\n",
    "        return handle_exception(\"Empty path isn't valid !\")\n",
    "\n",
    "    try:\n",
    "        workspace_path = data_repo.get_deep_model_info('workspace_path')\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "    \n",
    "    _depth_prob_map_dtype = {'16-bits': np.dtype('float16'), '32-bits': np.dtype('float32')}\n",
    "    images_list = [*data_repo.get_all_inference_keys()]\n",
    "    images_props = [*data_repo.get_all_inference_info()]\n",
    "    images_list_name = [*data_repo.get_all_inference_info()]\n",
    "    images_list_name = [x[\"filePath\"] for x in images_list_name]\n",
    "    output_folder = output[\"outputPath\"]\n",
    "    model_file_h5 = os.path.join(workspace_path, \"frozen\", network + \".meta.h5\")\n",
    "    model_file = os.path.join(workspace_path, \"frozen\", network)    \n",
    "    error_message = \"\"\n",
    "    try:\n",
    "        metadata = dataset.load_metadata(model_file_h5)\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    patch_size = metadata['patch_size']\n",
    "    num_classes = metadata.get('num_classes', 2)\n",
    "\n",
    "    border = (patches[\"patchBorder\"][0], patches[\"patchBorder\"][1], patches[\"patchBorder\"][2])\n",
    "    padding = (patches[\"volumePadding\"][0], patches[\"volumePadding\"][1], patches[\"volumePadding\"][2])\n",
    "\n",
    "    if (len(images_list) > 0):\n",
    "        if (any(np.array(padding) > np.array(patch_size))):\n",
    "            error_message = 'One of Volume Padding axis is greater than Patch Size axis'\n",
    "        elif (any(np.array(border) > np.array(patch_size))):\n",
    "            error_message = 'One of Patch Border axis is greater than Patch Size axis'\n",
    "        elif (output[\"probabilityMap\"] == False and output[\"label\"] == False):\n",
    "            error_message = 'Please select the output type'\n",
    "        else:\n",
    "            if output_folder:\n",
    "                logging.debug('{}'.format(output_folder))\n",
    "\n",
    "            else:\n",
    "                error_message = 'Please specify the output path.'\n",
    "\n",
    "    else:\n",
    "        error_message = 'Please specify the list of images to segment.'\n",
    "\n",
    "    if (error_message != \"\"):\n",
    "        _debugger_print(\"error in run_inference func\", error_message)\n",
    "        return handle_exception(error_message)\n",
    "\n",
    "    batch_size = metadata['batch_size']\n",
    "    input_node = metadata['input_node']\n",
    "    output_node = metadata['output_node']\n",
    "\n",
    "    mean = np.float32(metadata['mean'])\n",
    "    std = np.float32(metadata['std'])\n",
    "\n",
    "    logging.debug('images_list: {}'.format(images_list))\n",
    "    logging.debug('images_props: {}'.format(images_props))\n",
    "\n",
    "    gpus = data_repo.get_inference_gpus()\n",
    "\n",
    "    inference_controller = InferenceController(\"\",\n",
    "                                               \",\".join(map(str, gpus)),\n",
    "                                               use_tensorrt=isInferenceOpChecked)\n",
    "\n",
    "    inference_controller.load_graph(model_file, input_node + \":0\", output_node + \":0\")\n",
    "\n",
    "    try:\n",
    "        inference_controller.optimize_batch((batch_size, *patch_size),\n",
    "                                            border,\n",
    "                                            padding=padding,\n",
    "                                            num_classes=num_classes)\n",
    "    except:\n",
    "        return handle_exception(\"Not enough GPU memory to use in inference\")\n",
    "\n",
    "    for image_file_name, image_file, image_props_file in zip(images_list_name, images_list, images_props):\n",
    "        f, _ = os.path.splitext(os.path.basename(image_file_name))\n",
    "        data = data_repo.get_inference_data(image_file)\n",
    "        image_props = {\n",
    "            \"shape\": [image_props_file[\"shape\"][0], image_props_file[\"shape\"][1], image_props_file[\"shape\"][2]],\n",
    "            \"dtype\": data.dtype}\n",
    "\n",
    "        t1 = time.time()\n",
    "        t2 = time.time()\n",
    "        logging.debug('Read image: {}'.format(t2 - t1))\n",
    "        t1 = time.time()\n",
    "        # optimize to avoid unecessary copy\n",
    "        logging.debug('{}'.format(data.shape))\n",
    "        data = standardize(data, mean, std, 64)\n",
    "        t2 = time.time()\n",
    "        logging.debug('Rotate and cast image: {}'.format(t2 - t1)) \n",
    "        dtype = _depth_prob_map_dtype[output[\"outputBits\"]]\n",
    "\n",
    "        output_data = inference_controller.inference(data, output_dtype=dtype)\n",
    "\n",
    "        try:\n",
    "            image.save_inference(output_folder,\n",
    "                                 f,\n",
    "                                 output_data,\n",
    "                                 num_classes,\n",
    "                                 image_props,\n",
    "                                 save_prob_map=output[\"probabilityMap\"],\n",
    "                                 save_label=output[\"label\"],\n",
    "                                 output_dtype=dtype,\n",
    "                                 ext=output[\"outputExt\"][1:])\n",
    "        except Exception as e:\n",
    "            return handle_exception(\"Error to save the inference : {}\".format(str(e)))\n",
    "\n",
    "    return jsonify(\"successes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686b8b7",
   "metadata": {},
   "source": [
    "# network module functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0319ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_networks():\n",
    "    \"\"\"\n",
    "    Access workspace on data repo, set up network controller and reads available nets\n",
    "    find workspace > creates a network controller, reads available networks and its custom parametes\n",
    "    \n",
    "    return available networks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workspace_path = data_repo.get_deep_model_info(key='workspace_path')\n",
    "    except Exception as e:\n",
    "        return handle_exception('Workspace path not found. {}'.format(str(e)))\n",
    "    \n",
    "    try:\n",
    "        network_controller = HostNetworkController(workspace=workspace_path, streaming_mode=True)\n",
    "    except Exception as e:\n",
    "        return handle_exception('Unable to load Network Controller from this workspace. {}'.format(str(e)))\n",
    "    \n",
    "    return jsonify(network_controller.network_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "900a2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_params(network_name):\n",
    "    # {k: self._custom_params_ui[k].currentText() for k in self._custom_params_ui}\n",
    "    # params = self._network_controller.network_list_params(network)\n",
    "    \n",
    "    try:\n",
    "        workspace_path = data_repo.get_deep_model_info(key='workspace_path')\n",
    "    except Exception as e:\n",
    "        return handle_exception('Workspace path not found. {}'.format(str(e)))\n",
    "    \n",
    "    try:\n",
    "        network_controller = HostNetworkController(workspace=workspace_path, streaming_mode=True)\n",
    "    except Exception as e:\n",
    "        return handle_exception('Unable to load Network Controller from this workspace. {}'.format(str(e)))\n",
    "    \n",
    "    return jsonify(network_controller.network_list_params(network_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c1c5865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_network():\n",
    "    import_network_path = request.json['import_network_path']\n",
    "    import_network_name = request.json['import_network_name']\n",
    "\n",
    "    try:\n",
    "        workspace_path = data_repo.get_deep_model_info(key='workspace_path')\n",
    "    except Exception as e:\n",
    "        return handle_exception('Workspace path not found. {}'.format(str(e)))\n",
    "    \n",
    "    try:\n",
    "        network_controller = HostNetworkController(workspace=workspace_path, streaming_mode=True)\n",
    "    except Exception as e:\n",
    "        return handle_exception('Unable to load Network Controller from this workspace. {}'.format(str(e)))\n",
    "\n",
    "    try:\n",
    "        network_controller.import_model(import_network_path, import_network_name)\n",
    "    except Exception as e:\n",
    "        return handle_exception('Unable to import network. {}'.format(str(e)))\n",
    "\n",
    "    return get_network_params(import_network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4d20372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667af63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_cache_path(path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_network_custom_parameters(custom_params):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bd7a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_network_parameters(parameters):\n",
    "    _set_cache_path(parameters['path'])\n",
    "    _set_network_custom_parameters(parameters['custom_params'])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "524dd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_tensorboard():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "352914d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3378abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "32a6f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_network(network_name):\n",
    "    \"\"\"\n",
    "        path should have extension '.model.tar.gz'\n",
    "    \"\"\"\n",
    "\n",
    "    export_network_path = request.json['export_network_path']\n",
    "    \n",
    "    try:\n",
    "        workspace_path = data_repo.get_deep_model_info(key='workspace_path')\n",
    "    except Exception as e:\n",
    "        return handle_exception('Workspace path not found. {}'.format(str(e)))\n",
    "    \n",
    "    try:\n",
    "        network_controller = HostNetworkController(workspace=workspace_path, streaming_mode=True)\n",
    "    except Exception as e:\n",
    "        return handle_exception('Unable to load Network Controller from this workspace. {}'.format(str(e)))\n",
    "    \n",
    "    if network_name not in network_controller.network_models:\n",
    "        return handle_exception('Network name not in network models list. {}'.format(str(e)))\n",
    "\n",
    "    try:\n",
    "        network_controller.export_model(network_name, export_network_path)\n",
    "    except Exception as e:\n",
    "        return handle_exception('Unable to export network. {} {}'.format(str(e), export_network_path))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "593316b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_inference():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2840a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# host mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7f4fc",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "128ce1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "dspath = '/home/brunocarlos_lnls/work/images/datasets/dataset_augmented.h5'\n",
    "wspcPath = '/home/brunocarlos_lnls/work/anot/temp/web/workspace'\n",
    "tmpPath = wspcPath+'/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30251149",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mworkspace_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "request.json['workspace_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16b0a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we have to load the workspace in data_repo, which is done in other module of the frontend\n",
    "# setting workspace\n",
    "\n",
    "request.set_json({'workspace_path': wspcPath})\n",
    "load_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff38ef4",
   "metadata": {},
   "source": [
    "looking if the workspace path is written on data_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a812949f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/brunocarlos_lnls/work/anot/temp/web/workspace'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_repo.get_deep_model_info(key='workspace_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82682d1a",
   "metadata": {},
   "source": [
    "testing other functions from deep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2cc8acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 13:10:32.414962: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-01 13:10:32.469246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-01 13:10:32.491433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-01 13:10:32.491910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-01 13:10:32.877459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-01 13:10:32.877751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-01 13:10:32.877965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-01 13:10:32.878176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 1388 MB memory:  -> device: 0, name: NVIDIA GeForce MX450, pci bus id: 0000:05:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'key': 0, 'value': 'GPU 0', 'label': '/device:GPU:0'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c19347",
   "metadata": {},
   "source": [
    "testing a functions written in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "31512dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newvnet', 'unet3d', 'vnet', 'unet2d']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_networks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6b1908ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Patch Size': {'type': 'Group',\n",
       "  'value': {'xy': {'type': 'Combo',\n",
       "    'argname': 'patch_size__xy',\n",
       "    'value': ['32,32', '64,64', '128,128', '256,256', '512,512']},\n",
       "   'z': {'type': 'Combo',\n",
       "    'argname': 'patch_size__z',\n",
       "    'value': ['16', '32', '64', '128', '256', '512']}}},\n",
       " 'Drop Classifier': {'type': 'Combo',\n",
       "  'argname': 'drop_classifier',\n",
       "  'value': ['No', 'Yes']}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_network('newvnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b391449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "request.append_json({'import_network_name':'newvnet',\n",
    "                  'import_network_path':'/home/brunocarlos_lnls/work/anot/temp/legacy/frozen/my_frozen_vnet.model.tar.gz'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c9c93188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Patch Size': {'type': 'Group',\n",
       "  'value': {'xy': {'type': 'Combo',\n",
       "    'argname': 'patch_size__xy',\n",
       "    'value': ['32,32', '64,64', '128,128', '256,256', '512,512']},\n",
       "   'z': {'type': 'Combo',\n",
       "    'argname': 'patch_size__z',\n",
       "    'value': ['16', '32', '64', '128', '256', '512']}}},\n",
       " 'Drop Classifier': {'type': 'Combo',\n",
       "  'argname': 'drop_classifier',\n",
       "  'value': ['No', 'Yes']}}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdd27e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0367ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "021ca3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workspace_path': '/home/brunocarlos_lnls/work/anot/temp/web/workspace',\n",
       " 'import_network_path': 'newvnet',\n",
       " 'import_network_name': '/home/brunocarlos_lnls/work/anot/temp/legacy/frozen/my_frozen_vnet.model.tar.gz',\n",
       " 'export_network_path': '/home/brunocarlos_lnls/work/anot/temp/web/workspace/frozen/any_name.model.tar.gz',\n",
       " 'export_network_name': 'frozenvnet'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request.append_json({'export_network_name':'frozenvnet',\n",
    "                  'export_network_path':'/home/brunocarlos_lnls/work/anot/temp/web/workspace/frozen/any_name.model.tar.gz'})\n",
    "request.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4062fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Prune things away ...\n",
      "/home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/models/checkpoint\n",
      "No checkpoint found\n",
      "folder:  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d\n",
      "exported files:  ['.dockerignore', 'docker_connect', 'Dockerfile.ppc64le', 'params.json', 'docker_destroy', 'Dockerfile.x86_64', 'config.cfg', 'Dockerfile', 'default.cfg', 'docker_create', 'train/label/.gitkeep', 'train/data/.gitkeep', 'label/.gitkeep', 'utils/__init__.py', 'utils/sampling.py', 'utils/image.py', 'utils/info.py', 'infer/label/.gitkeep', 'infer/data/.gitkeep', 'bin/train', 'bin/version', 'bin/copy_train', 'bin/infer', 'bin/freeze', 'bin/track', 'bin/finetune', 'bin/install', 'niftynet/settings_training.txt', 'niftynet/label.csv', 'niftynet/dataset_split.csv', 'niftynet/train/.gitkeep', 'niftynet/extensions/config.ini.template', 'niftynet/extensions/__init__.py', 'niftynet/extensions/log.txt', 'niftynet/extensions/engine/__init__.py', 'niftynet/extensions/engine/custom_optimiser.py', 'niftynet/extensions/engine/custom_optimizer.py', 'niftynet/extensions/network/__init__.py']\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/.dockerignore .dockerignore\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/docker_connect docker_connect\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/Dockerfile.ppc64le Dockerfile.ppc64le\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/params.json params.json\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/docker_destroy docker_destroy\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/Dockerfile.x86_64 Dockerfile.x86_64\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/config.cfg config.cfg\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/Dockerfile Dockerfile\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/default.cfg default.cfg\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/docker_create docker_create\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/train/label/.gitkeep train/label/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/train/data/.gitkeep train/data/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/label/.gitkeep label/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/utils/__init__.py utils/__init__.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/utils/sampling.py utils/sampling.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/utils/image.py utils/image.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/utils/info.py utils/info.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/infer/label/.gitkeep infer/label/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/infer/data/.gitkeep infer/data/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/train bin/train\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/version bin/version\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/copy_train bin/copy_train\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/infer bin/infer\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/freeze bin/freeze\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/track bin/track\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/finetune bin/finetune\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/install bin/install\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/settings_training.txt niftynet/settings_training.txt\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/label.csv niftynet/label.csv\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/dataset_split.csv niftynet/dataset_split.csv\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/train/.gitkeep niftynet/train/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/config.ini.template niftynet/extensions/config.ini.template\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/__init__.py niftynet/extensions/__init__.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/log.txt niftynet/extensions/log.txt\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/engine/__init__.py niftynet/extensions/engine/__init__.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/engine/custom_optimiser.py niftynet/extensions/engine/custom_optimiser.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/engine/custom_optimizer.py niftynet/extensions/engine/custom_optimizer.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/network/__init__.py niftynet/extensions/network/__init__.py\n"
     ]
    }
   ],
   "source": [
    "export_network('unet2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc2ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
