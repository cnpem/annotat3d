{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65967224",
   "metadata": {},
   "source": [
    "functions needed for testing in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b128a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only works in jupyter-lab on webdev-clone conda env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebde659c-d3b4-4123-8384-c1046753bbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "dual-energy              /home/brunocarlos_lnls/.conda/envs/dual-energy\n",
      "gabor-seg                /home/brunocarlos_lnls/.conda/envs/gabor-seg\n",
      "legacy                   /home/brunocarlos_lnls/.conda/envs/legacy\n",
      "web-neuroglancer         /home/brunocarlos_lnls/.conda/envs/web-neuroglancer\n",
      "webdev                   /home/brunocarlos_lnls/.conda/envs/webdev\n",
      "webdev-clone          *  /home/brunocarlos_lnls/.conda/envs/webdev-clone\n",
      "base                     /opt/conda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1819636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<setuptools.extension.Extension('sscAnnotat3D.app') at 0x7f5f02d8c280>, <setuptools.extension.Extension('sscAnnotat3D.__version__') at 0x7f5f02ce0bb0>, <setuptools.extension.Extension('sscAnnotat3D.aux_functions') at 0x7f5f02ce0e20>, <setuptools.extension.Extension('sscAnnotat3D.utils') at 0x7f5f02ce0e50>, <setuptools.extension.Extension('sscAnnotat3D.__init__') at 0x7f5f02ce0e80>, <setuptools.extension.Extension('sscAnnotat3D.superpixels') at 0x7f5f02ce0eb0>, <setuptools.extension.Extension('sscAnnotat3D.label') at 0x7f5f02ce0ee0>, <setuptools.extension.Extension('sscAnnotat3D.binary') at 0x7f5f02ce0f10>, <setuptools.extension.Extension('sscAnnotat3D.progressbar') at 0x7f5f02ce0dc0>]\n",
      "[]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.repository.data_repo') at 0x7f5f02ce0df0>, <setuptools.extension.Extension('sscAnnotat3D.repository.__init__') at 0x7f5f02ce0400>, <setuptools.extension.Extension('sscAnnotat3D.repository.module_repo') at 0x7f5f02ce0d60>]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.api.superpixel') at 0x7f5f02ce0f40>, <setuptools.extension.Extension('sscAnnotat3D.api.io') at 0x7f5f02ce0c40>, <setuptools.extension.Extension('sscAnnotat3D.api.deep') at 0x7f5f02ce0fa0>, <setuptools.extension.Extension('sscAnnotat3D.api.__init__') at 0x7f5f02ce0f70>, <setuptools.extension.Extension('sscAnnotat3D.api.remotevis') at 0x7f5f02ce0430>, <setuptools.extension.Extension('sscAnnotat3D.api.annotation') at 0x7f5f02ce0fd0>, <setuptools.extension.Extension('sscAnnotat3D.api.filters') at 0x7f5f02cf4040>, <setuptools.extension.Extension('sscAnnotat3D.api.image') at 0x7f5f02cf4070>]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.api.modules.superpixel_segmentation_module') at 0x7f5f02cf40a0>, <setuptools.extension.Extension('sscAnnotat3D.api.modules.pixel_segmentation_module') at 0x7f5f02cf4100>, <setuptools.extension.Extension('sscAnnotat3D.api.modules.__init__') at 0x7f5f02cf40d0>]\n",
      "[]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.modules.annotation_module') at 0x7f5f02cf4130>, <setuptools.extension.Extension('sscAnnotat3D.modules.classifier_segmentation_module') at 0x7f5f02cf4190>, <setuptools.extension.Extension('sscAnnotat3D.modules.deep_network_module') at 0x7f5f02cf4160>, <setuptools.extension.Extension('sscAnnotat3D.modules.superpixel_segmentation_module') at 0x7f5f02cf41c0>, <setuptools.extension.Extension('sscAnnotat3D.modules.segmentation_module') at 0x7f5f02cf41f0>, <setuptools.extension.Extension('sscAnnotat3D.modules.pixel_segmentation_module') at 0x7f5f02cf4220>, <setuptools.extension.Extension('sscAnnotat3D.modules.__init__') at 0x7f5f02cf4250>]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.deeplearning.deeplearning_workspace_dialog') at 0x7f5f02cf4280>, <setuptools.extension.Extension('sscAnnotat3D.deeplearning.__init__') at 0x7f5f02cf42b0>]\n",
      "[<setuptools.extension.Extension('sscAnnotat3D.cython.__init__') at 0x7f5f02cf42e0>, <setuptools.extension.Extension('sscAnnotat3D.cython.annotation') at 0x7f5f02cf4340>, <setuptools.extension.Extension('sscAnnotat3D.cython.annotation') at 0x7f5f02cf4310>]\n",
      "/home/brunocarlos_lnls/.conda/envs/webdev-clone/lib/python3.9/site-packages/setuptools/dist.py:680: SetuptoolsDeprecationWarning: As setuptools moves its configuration towards `pyproject.toml`,\n",
      "`setuptools.config.parse_configuration` became deprecated.\n",
      "\n",
      "For the time being, you can use the `setuptools.config.setupcfg` module\n",
      "to access a backward compatible API, but this module is provisional\n",
      "and might be removed in the future.\n",
      "\n",
      "  parse_configuration(self, self.command_options,\n",
      "running install\n",
      "running build\n",
      "running build_py\n",
      "package init file 'sscAnnotat3D/colormaps/__init__.py' not found (or not a regular file)\n",
      "package init file 'sscAnnotat3D/static/__init__.py' not found (or not a regular file)\n",
      "package init file 'sscAnnotat3D/templates/__init__.py' not found (or not a regular file)\n",
      "running egg_info\n",
      "writing sscAnnotat3D.egg-info/PKG-INFO\n",
      "writing dependency_links to sscAnnotat3D.egg-info/dependency_links.txt\n",
      "writing requirements to sscAnnotat3D.egg-info/requires.txt\n",
      "writing top-level names to sscAnnotat3D.egg-info/top_level.txt\n",
      "reading manifest file 'sscAnnotat3D.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'sscAnnotat3D.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "running install_lib\n",
      "running install_egg_info\n",
      "removing '/home/brunocarlos_lnls/.conda/envs/webdev-clone/lib/python3.9/site-packages/sscAnnotat3D-1.3.0-py3.9.egg-info' (and everything under it)\n",
      "Copying sscAnnotat3D.egg-info to /home/brunocarlos_lnls/.conda/envs/webdev-clone/lib/python3.9/site-packages/sscAnnotat3D-1.3.0-py3.9.egg-info\n",
      "running install_scripts\n",
      "    _                      _        _   _____ ____\n",
      "   / \\   _ __  _ __   ___ | |_ __ _| |_|___ /|  _ \\\n",
      "  / _ \\ | '_ \\| '_ \\ / _ \\| __/ _` | __| |_ \\| | | |\n",
      " / ___ \\| | | | | | | (_) | || (_| | |_ ___) | |_| |\n",
      "/_/   \\_\\_| |_|_| |_|\\___/ \\__\\__,_|\\__|____/|____/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd ../../ && python setup.py install && cd sscAnnotat3D/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e8f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prop functions and objects for prototyping in this notebook\n",
    "\"\"\"\n",
    "def jsonify(obj):\n",
    "    # overwriting for tests using jupyter\n",
    "    return obj\n",
    "\n",
    "def handle_exception(msg):\n",
    "    print('error_msg: {} '.format(msg))\n",
    "\n",
    "class requestClass():\n",
    "    def __init__(self):\n",
    "        self.json = None\n",
    "\n",
    "    def set_json(self, newdict):\n",
    "        self.json = newdict\n",
    "        \n",
    "    def append_json(self, newdict):\n",
    "        self.json = {**self.json, **newdict}\n",
    "\n",
    "    def clear(self):\n",
    "        self.json = None\n",
    "\n",
    "global request \n",
    "request = requestClass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968c5f9",
   "metadata": {},
   "source": [
    "copying deep.py and deactivating flask functionalities and derived functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797b05a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This script contains some back-end functions for the deep learning module\n",
    "\n",
    "@authors : Gabriel Borin Macedo (gabriel.macedo@lnls.br or borinmacedo@gmail.com)\n",
    "         : Bruno Carlos (bruno.carlos@lnls.br)\n",
    "\n",
    "TODO : Don't forget to document the functions\n",
    "\n",
    "\"\"\"\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# from flask import Blueprint, jsonify, request\n",
    "# from flask_cors import cross_origin\n",
    "from werkzeug.exceptions import BadRequest\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sscIO.io import read_volume\n",
    "from sscDeepsirius.cython import standardize\n",
    "from sscAnnotat3D.repository import data_repo\n",
    "from sscDeepsirius.utils import dataset, image, augmentation\n",
    "from sscDeepsirius.controller.inference_controller import InferenceController\n",
    "from sscDeepsirius.controller.host_network_controller import HostNetworkController \n",
    "from sscAnnotat3D.deeplearning import DeepLearningWorkspaceDialog\n",
    "\n",
    "\n",
    "# app = Blueprint('deep', __name__)\n",
    "\n",
    "def init_logger(init_msg : str = '\\nStarting message logger queue.\\n'):\n",
    "    data_repo.init_logger(init_msg)\n",
    "\n",
    "def log_msg(msg):\n",
    "    data_repo.set_log_message(msg)\n",
    "\n",
    "\n",
    "# @app.route(\"/read_log_queue\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def read_log_queue():\n",
    "    \"\"\"\n",
    "        Reads from a queue of messages stored on data_repo. \n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "            (str): An empty string if the queue is empty.\n",
    "    \"\"\"\n",
    "    msg = data_repo.dequeue_log_message()\n",
    "    if msg == None:\n",
    "        return ''\n",
    "    \n",
    "    return msg\n",
    "\n",
    "\n",
    "# @app.errorhandler(BadRequest)\n",
    "def handle_exception(error_msg: str):\n",
    "    \"\"\"\n",
    "    Function to handle error exception and returns to the user\n",
    "\n",
    "    Args:\n",
    "        error_msg (str): variable that contains the error\n",
    "\n",
    "    Returns:\n",
    "        (tuple): This function returns a tuple that contains the error as a JSON and an int 400\n",
    "\n",
    "    \"\"\"\n",
    "    return jsonify({\"error_msg\": error_msg}), 400\n",
    "\n",
    "\n",
    "# app.register_error_handler(400, handle_exception)\n",
    "\n",
    "\n",
    "def _convert_dtype_to_str(img_dtype: np.dtype):\n",
    "    \"\"\"\n",
    "    Build-in function to convert dtype to a str\n",
    "\n",
    "    Args:\n",
    "        img_dtype (np.dtype): np.dtype object that contains\n",
    "\n",
    "    Returns:\n",
    "        (str): returns the str version of the dtype\n",
    "\n",
    "    \"\"\"\n",
    "    return np.dtype(img_dtype).name\n",
    "\n",
    "\n",
    "def _debugger_print(msg: str, payload: any):\n",
    "    \"\"\"\n",
    "    Build-in function to user as debugger\n",
    "\n",
    "    Args:\n",
    "        msg(str): string message to user in debugger\n",
    "        payload(any): a generic payload\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"\\n----------------------------------------------------------\")\n",
    "    print(\"{} : {}\".format(msg, payload))\n",
    "    print(\"----------------------------------------------------------\\n\")\n",
    "\n",
    "# removed from io.py and placed here\n",
    "# @app.route(\"/open_new_workspace\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def open_new_workspace():\n",
    "    \"\"\"\n",
    "    Function that opens a new workspace\n",
    "\n",
    "    Notes:\n",
    "        the request.json[\"workspace_path\"] receives only the parameter \"selected_labels\"(str)\n",
    "\n",
    "    Returns:\n",
    "        (str): returns a string that contains the new workspace path\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workspace_path = request.json[\"workspace_path\"]\n",
    "        workspace_root = request.json[\"workspace_root\"]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    if (workspace_root == \"\"):\n",
    "        return handle_exception(\"Empty path isn't valid !\")\n",
    "\n",
    "    deep_model = DeepLearningWorkspaceDialog()\n",
    "    save_status, error_desc = deep_model.open_new_workspace(workspace_path)\n",
    "\n",
    "    if (save_status):\n",
    "        data_repo.set_deep_model_info(key='workspace_path', data=workspace_path)\n",
    "        return jsonify(workspace_path)\n",
    "\n",
    "    return handle_exception(\"unable to create the Workspace ! : {}\".format(error_desc))\n",
    "\n",
    "\n",
    "# @app.route(\"/load_workspace\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def load_workspace():\n",
    "    \"\"\"\n",
    "    Function that loads a created workspace\n",
    "\n",
    "    Notes:\n",
    "        the request.json[\"workspace_path\"] receives only the parameter \"selected_labels\"(str)\n",
    "\n",
    "    Returns:\n",
    "        (str): returns a string that contains the loaded workspace path\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workspace_path = request.json[\"workspace_path\"]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    deep_model = DeepLearningWorkspaceDialog()\n",
    "    check_valid_workspace = deep_model.check_workspace(workspace_path)\n",
    "\n",
    "    if (check_valid_workspace):\n",
    "        data_repo.set_deep_model_info(key='workspace_path', data=workspace_path)\n",
    "        return jsonify(check_valid_workspace)\n",
    "\n",
    "    return handle_exception(\"path \\\"{}\\\" is a invalid workspace path!\".format(workspace_path))\n",
    "\n",
    "\n",
    "def _augmenters_list(augmentation_vec: list = None, ion_range_vec: list = None):\n",
    "    \"\"\"\n",
    "    Build-in function that formats the front-end data into the correct structure in the function augment_web\n",
    "\n",
    "    Args:\n",
    "        augmentation_vec (list): a list that contains all the elements to augment\n",
    "        ion_range_vec (list): a list that contains the value of ion-rage of some augment parameters\n",
    "\n",
    "    Returns:\n",
    "        (list): returns a list that contains the params to augment and the respective values\n",
    "\n",
    "    \"\"\"\n",
    "    augmenters = []\n",
    "\n",
    "    # vertical-flip option\n",
    "    if (augmentation_vec[0][\"isChecked\"]):\n",
    "        augmenters.append(\"vertical-flip\")\n",
    "\n",
    "    # horizontal-flip option\n",
    "    if (augmentation_vec[1][\"isChecked\"]):\n",
    "        augmenters.append(\"horizontal-flip\")\n",
    "\n",
    "    # rotate-90-degrees option\n",
    "    if (augmentation_vec[2][\"isChecked\"]):\n",
    "        augmenters.append(\"rotate-90-degrees\")\n",
    "\n",
    "    # rotate-less-90-degrees (rotate 270 degrees) option\n",
    "    if (augmentation_vec[3][\"isChecked\"]):\n",
    "        augmenters.append(\"rotate-less-90-degrees\")\n",
    "\n",
    "    # contrast option\n",
    "    if (augmentation_vec[4][\"isChecked\"]):\n",
    "        # c_min, c_max = ion_range_vec[0][\"ionRangeLimit\"].values\n",
    "        c_lower, c_upper = ion_range_vec[0][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"contrast\",\n",
    "                 contrast=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in contrast\", augmentation_vec[-1])\n",
    "\n",
    "    # linear-contrast option\n",
    "    if (augmentation_vec[5][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[1][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"linear-contrast\",\n",
    "                 linearContrast=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in linear-contrast\", augmentation_vec[-1])\n",
    "\n",
    "    # dropout option\n",
    "    if (augmentation_vec[6][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[2][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"dropout\",\n",
    "                 dropout=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in dropout\", augmentation_vec[-1])\n",
    "\n",
    "    # gaussian-blur option\n",
    "    if (augmentation_vec[7][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[3][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"gaussian-blur\",\n",
    "                 sigma=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in gaussian-blur\", augmentation_vec[-1])\n",
    "\n",
    "    # average-blur option\n",
    "    if (augmentation_vec[8][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[4][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"average-blur\",\n",
    "                 k=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in average-blur\", augmentation_vec[-1])\n",
    "\n",
    "    # additive-poisson-noise option\n",
    "    if (augmentation_vec[9][\"isChecked\"]):\n",
    "        c_lower, c_upper = ion_range_vec[5][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"additive-poisson-noise\",\n",
    "                 k=(c_lower, c_upper)))\n",
    "        _debugger_print(\"test for ion-range in additive-poisson-noise\", augmentation_vec[-1])\n",
    "\n",
    "    # elastic-deformation option\n",
    "    if (augmentation_vec[10][\"isChecked\"]):\n",
    "        c_lower_alpha, c_upper_alpha = ion_range_vec[6][\"actualRangeVal\"].values()\n",
    "        c_lower_sigma, c_upper_sigma = ion_range_vec[7][\"actualRangeVal\"].values()\n",
    "        augmenters.append(\n",
    "            dict(type=\"elastic-deformation\",\n",
    "                 alpha=(c_lower_alpha, c_upper_alpha),\n",
    "                 sigma=(c_lower_sigma, c_upper_sigma)))\n",
    "        _debugger_print(\"test for ion-range in elastic-deformation\", augmentation_vec[-1])\n",
    "\n",
    "    _debugger_print(\"augmenters param\", augmenters)\n",
    "    return augmenters\n",
    "\n",
    "\n",
    "# @app.route(\"/create_dataset\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Function that creates the .h5 dataset\n",
    "\n",
    "    Notes:\n",
    "        This function is used in DatasetComp.tsx\n",
    "\n",
    "    Returns:\n",
    "        (dict): returns a dict that contains the dataset .h5 name. Otherwise, will return an error for the front-end\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        output = request.json[\"file_path\"]\n",
    "        sample = request.json[\"sample\"]\n",
    "        augmentation_vec = request.json[\"augmentation\"]\n",
    "        ion_range_vec = request.json[\"ion_range_vec\"]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    size = (sample[\"patchSize\"][0], sample[\"patchSize\"][1], sample[\"patchSize\"][2])\n",
    "    num_classes = sample[\"nClasses\"]\n",
    "    nsamples = sample[\"sampleSize\"]\n",
    "    offset = (0, 0, 0)\n",
    "    logging.debug('size = {}, nsamples = {}'.format(size, sample[\"sampleSize\"]))\n",
    "    augmenter_params = _augmenters_list(augmentation_vec, ion_range_vec)\n",
    "\n",
    "    imgs = list(data_repo.get_all_dataset_data().values())\n",
    "    labels = list(data_repo.get_all_dataset_label().values())\n",
    "    weights = list(data_repo.get_all_dataset_weight().values())\n",
    "\n",
    "    data, error_status = dataset.create_dataset_web(imgs, labels, weights,\n",
    "                                            output, nsamples, num_classes,\n",
    "                                            size, offset)\n",
    "\n",
    "    if (not data):\n",
    "        return handle_exception(error_status)\n",
    "\n",
    "    initial_output = output\n",
    "    splited_str = output.split(\"/\")\n",
    "    dataset_name = splited_str[-1]\n",
    "    new_dataset_name = splited_str[-1].split(\".\")[0] + \"_augment\" + \".h5\"\n",
    "    output = output.replace(dataset_name, new_dataset_name)\n",
    "\n",
    "    if (augmenter_params):\n",
    "        data, error_status = augmentation.augment_web(output, initial_output, augmenter_params, data)\n",
    "\n",
    "    if (not data):\n",
    "        return handle_exception(error_status)\n",
    "\n",
    "    dataset.save_dataset(data)\n",
    "\n",
    "    return jsonify({\"datasetFilename\": initial_output.split(\"/\")[-1]})\n",
    "\n",
    "\n",
    "# @app.route(\"/open_inference_files/<file_id>\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def open_inference_files(file_id: str):\n",
    "    _debugger_print(\"new key\", file_id)\n",
    "    try:\n",
    "        file_path = request.json[\"image_path\"]\n",
    "    except:\n",
    "        return handle_exception(\"Error while trying to get the image path\")\n",
    "\n",
    "    try:\n",
    "        file_dtype = request.json[\"image_dtype\"]\n",
    "    except:\n",
    "        return handle_exception(\"Error while trying to get the image dtype\")\n",
    "\n",
    "    file = file_path.split(\"/\")[-1]\n",
    "    file_name, extension = os.path.splitext(file)\n",
    "\n",
    "    if (file == \"\"):\n",
    "        return handle_exception(\"Empty path isn't valid !\")\n",
    "\n",
    "    raw_extensions = [\".raw\", \".b\"]\n",
    "    tif_extensions = [\".tif\", \".tiff\", \".npy\", \".cbf\"]\n",
    "\n",
    "    extensions = [*raw_extensions, *tif_extensions]\n",
    "\n",
    "    if extension not in extensions:\n",
    "        return handle_exception(\"The extension {} isn't supported !\".format(extension))\n",
    "\n",
    "    error_msg = \"\"\n",
    "\n",
    "    try:\n",
    "        use_image_raw_parse = request.json[\"use_image_raw_parse\"]\n",
    "        if (extension in tif_extensions or use_image_raw_parse):\n",
    "            start = time.process_time()\n",
    "            image, info = read_volume(file_path, 'numpy')\n",
    "            end = time.process_time()\n",
    "            error_msg = \"No such file or directory {}\".format(file_path)\n",
    "\n",
    "            if (_convert_dtype_to_str(image.dtype) != file_dtype and (file_id == \"image\" or file_id == \"label\")):\n",
    "                image = image.astype(file_dtype)\n",
    "\n",
    "        else:\n",
    "            image_raw_shape = request.json[\"image_raw_shape\"]\n",
    "            start = time.process_time()\n",
    "            image, info = read_volume(file_path, 'numpy',\n",
    "                                               shape=(image_raw_shape[2], image_raw_shape[1], image_raw_shape[0]),\n",
    "                                               dtype=file_dtype)\n",
    "            end = time.process_time()\n",
    "            error_msg = \"Unable to reshape the volume {} into shape {} and type {}. \" \\\n",
    "                        \"Please change the dtype and shape and load the image again\".format(file, request.json[\n",
    "                \"image_raw_shape\"], file_dtype)\n",
    "        image_shape = image.shape\n",
    "        image_dtype = _convert_dtype_to_str(image.dtype)\n",
    "    except:\n",
    "        return handle_exception(error_msg)\n",
    "\n",
    "    image_info = {\"fileName\": file_name + extension,\n",
    "                  \"shape\": image_shape,\n",
    "                  \"type\": image_dtype,\n",
    "                  \"scan\": info,\n",
    "                  \"time\": np.round(end - start, 2),\n",
    "                  \"size\": np.round(image.nbytes / 1000000, 2),\n",
    "                  \"filePath\": file_path}\n",
    "\n",
    "    data_repo.set_inference_data(key=file_id, data=image)\n",
    "    data_repo.set_inference_info(key=file_id, data=image_info)\n",
    "\n",
    "    return jsonify(image_info)\n",
    "\n",
    "\n",
    "# @app.route(\"/close_inference_file/<file_id>\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def close_inference_file(file_id: str):\n",
    "    \"\"\"\n",
    "    Function that deletes a file in inference a menu using a key as string\n",
    "\n",
    "    Args:\n",
    "        file_id (str): string used as key to delete the file\n",
    "\n",
    "    Returns:\n",
    "        (str): Returns a string that contains the error or \"success on delete the key i in Input Image inference\"\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_repo.del_inference_data(file_id)\n",
    "        data_repo.del_inference_info(file_id)\n",
    "        return jsonify(\"success on delete the key {} in Input Image inference\".format(file_id))\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "\n",
    "# @app.route(\"/close_all_files_dataset\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def close_all_inference_files():\n",
    "    \"\"\"\n",
    "    Function that delete all the files in inference menu\n",
    "\n",
    "    Returns:\n",
    "        (str): Returns a string that contains the error or \"Success to delete all data\"\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_repo.del_all_inference_data()\n",
    "        data_repo.del_all_inference_info()\n",
    "        return jsonify(\"Success to delete all data\")\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "\n",
    "\n",
    "# @app.route(\"/get_available_gpus\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "    Function that verify all the available gpus for inference and show to the user\n",
    "\n",
    "    Returns:\n",
    "        (dict): returns a dict that contains all the gpus to use for inference\n",
    "\n",
    "    \"\"\"\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    list_devices = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "    gpus = []\n",
    "    gpu_device_names = []\n",
    "    i = 0\n",
    "    for device in list_devices:\n",
    "        gpu_number = int(device.split(\":\")[-1])\n",
    "        gpus.append(gpu_number)\n",
    "        gpu_device_names.append({\n",
    "            \"key\": i,\n",
    "            \"value\": \"GPU {}\".format(gpu_number),\n",
    "            \"label\": device\n",
    "        })\n",
    "        i+=1\n",
    "\n",
    "    data_repo.set_inference_gpus(gpus)\n",
    "    return jsonify(gpu_device_names) \n",
    "\n",
    "\n",
    "# @app.route(\"/get_frozen_data\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def get_frozen_data():\n",
    "    \"\"\"\n",
    "    Function that verify all the frozen data in frozen directory created in the workspace menu\n",
    "\n",
    "    Returns:\n",
    "        (dict): returns a dict with all the meta_files for the user to choose and use in inference\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workspace_path = data_repo.get_deep_model_info('workspace_path')\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    try:\n",
    "        frozen_path_pb = glob.glob(workspace_path + \"frozen/*.pb\")\n",
    "        frozen_path_PB = glob.glob(workspace_path + \"frozen/*.PB\")\n",
    "        frozen_path = [*frozen_path_pb, *frozen_path_PB]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    meta_files = []\n",
    "\n",
    "    for i in range(0, len(frozen_path)):\n",
    "        file_name = frozen_path[i].split(\"/\")[-1]\n",
    "        meta_files.append({\n",
    "            \"key\": i,\n",
    "            \"value\": file_name,\n",
    "            \"label\": file_name\n",
    "        })\n",
    "\n",
    "    return jsonify(meta_files)\n",
    "\n",
    "\n",
    "# @app.route(\"/run_inference\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def run_inference():\n",
    "    \"\"\"\n",
    "    Function that run the inference\n",
    "\n",
    "    Returns:\n",
    "        (str): returns a string \"successes\" if the operation occurs without any error and an exception otherwise\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = request.json[\"output\"]\n",
    "        patches = request.json[\"patches\"]\n",
    "        network = request.json[\"network\"]\n",
    "        isInferenceOpChecked = request.json[\"isInferenceOpChecked\"]\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    if (output[\"outputPath\"] == \"\"):\n",
    "        return handle_exception(\"Empty path isn't valid !\")\n",
    "\n",
    "    try:\n",
    "        workspace_path = data_repo.get_deep_model_info('workspace_path')\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "    \n",
    "    _depth_prob_map_dtype = {'16-bits': np.dtype('float16'), '32-bits': np.dtype('float32')}\n",
    "    images_list = [*data_repo.get_all_inference_keys()]\n",
    "    images_props = [*data_repo.get_all_inference_info()]\n",
    "    images_list_name = [*data_repo.get_all_inference_info()]\n",
    "    images_list_name = [x[\"filePath\"] for x in images_list_name]\n",
    "    output_folder = output[\"outputPath\"]\n",
    "    model_file_h5 = os.path.join(workspace_path, \"frozen\", network + \".meta.h5\")\n",
    "    model_file = os.path.join(workspace_path, \"frozen\", network)    \n",
    "    error_message = \"\"\n",
    "    try:\n",
    "        metadata = dataset.load_metadata(model_file_h5)\n",
    "    except Exception as e:\n",
    "        return handle_exception(str(e))\n",
    "\n",
    "    patch_size = metadata['patch_size']\n",
    "    num_classes = metadata.get('num_classes', 2)\n",
    "\n",
    "    border = (patches[\"patchBorder\"][0], patches[\"patchBorder\"][1], patches[\"patchBorder\"][2])\n",
    "    padding = (patches[\"volumePadding\"][0], patches[\"volumePadding\"][1], patches[\"volumePadding\"][2])\n",
    "\n",
    "    if (len(images_list) > 0):\n",
    "        if (any(np.array(padding) > np.array(patch_size))):\n",
    "            error_message = 'One of Volume Padding axis is greater than Patch Size axis'\n",
    "        elif (any(np.array(border) > np.array(patch_size))):\n",
    "            error_message = 'One of Patch Border axis is greater than Patch Size axis'\n",
    "        elif (output[\"probabilityMap\"] == False and output[\"label\"] == False):\n",
    "            error_message = 'Please select the output type'\n",
    "        else:\n",
    "            if output_folder:\n",
    "                logging.debug('{}'.format(output_folder))\n",
    "\n",
    "            else:\n",
    "                error_message = 'Please specify the output path.'\n",
    "\n",
    "    else:\n",
    "        error_message = 'Please specify the list of images to segment.'\n",
    "\n",
    "    if (error_message != \"\"):\n",
    "        _debugger_print(\"error in run_inference func\", error_message)\n",
    "        return handle_exception(error_message)\n",
    "\n",
    "    batch_size = metadata['batch_size']\n",
    "    input_node = metadata['input_node']\n",
    "    output_node = metadata['output_node']\n",
    "\n",
    "    mean = np.float32(metadata['mean'])\n",
    "    std = np.float32(metadata['std'])\n",
    "\n",
    "    logging.debug('images_list: {}'.format(images_list))\n",
    "    logging.debug('images_props: {}'.format(images_props))\n",
    "\n",
    "    gpus = data_repo.get_inference_gpus()\n",
    "\n",
    "    inference_controller = InferenceController(\"\",\n",
    "                                               \",\".join(map(str, gpus)),\n",
    "                                               use_tensorrt=isInferenceOpChecked)\n",
    "\n",
    "    inference_controller.load_graph(model_file, input_node + \":0\", output_node + \":0\")\n",
    "\n",
    "    try:\n",
    "        inference_controller.optimize_batch((batch_size, *patch_size),\n",
    "                                            border,\n",
    "                                            padding=padding,\n",
    "                                            num_classes=num_classes)\n",
    "    except:\n",
    "        return handle_exception(\"Not enough GPU memory to use in inference\")\n",
    "\n",
    "    for image_file_name, image_file, image_props_file in zip(images_list_name, images_list, images_props):\n",
    "        f, _ = os.path.splitext(os.path.basename(image_file_name))\n",
    "        data = data_repo.get_inference_data(image_file)\n",
    "        image_props = {\n",
    "            \"shape\": [image_props_file[\"shape\"][0], image_props_file[\"shape\"][1], image_props_file[\"shape\"][2]],\n",
    "            \"dtype\": data.dtype}\n",
    "\n",
    "        t1 = time.time()\n",
    "        t2 = time.time()\n",
    "        logging.debug('Read image: {}'.format(t2 - t1))\n",
    "        t1 = time.time()\n",
    "        # optimize to avoid unecessary copy\n",
    "        logging.debug('{}'.format(data.shape))\n",
    "        data = standardize(data, mean, std, 64)\n",
    "        t2 = time.time()\n",
    "        logging.debug('Rotate and cast image: {}'.format(t2 - t1)) \n",
    "        dtype = _depth_prob_map_dtype[output[\"outputBits\"]]\n",
    "\n",
    "        output_data = inference_controller.inference(data, output_dtype=dtype)\n",
    "\n",
    "        try:\n",
    "            image.save_inference(output_folder,\n",
    "                                 f,\n",
    "                                 output_data,\n",
    "                                 num_classes,\n",
    "                                 image_props,\n",
    "                                 save_prob_map=output[\"probabilityMap\"],\n",
    "                                 save_label=output[\"label\"],\n",
    "                                 output_dtype=dtype,\n",
    "                                 ext=output[\"outputExt\"][1:])\n",
    "        except Exception as e:\n",
    "            return handle_exception(\"Error to save the inference : {}\".format(str(e)))\n",
    "\n",
    "    return jsonify(\"successes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819f079",
   "metadata": {},
   "source": [
    "# network module functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb511f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_network_controller():\n",
    "    \"\"\"\n",
    "    Access workspace on data repo, set up network controller and reads available nets\n",
    "    find workspace > creates a network controller, reads available networks and its custom parametes\n",
    "    \n",
    "    return available networks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workspace_path = data_repo.get_deep_model_info(key='workspace_path')\n",
    "    except Exception as e:\n",
    "        return handle_exception('Workspace path not found. {}'.format(str(e)))\n",
    "    \n",
    "    try:\n",
    "        network_controller = HostNetworkController(workspace=workspace_path, streaming_mode=True)\n",
    "    except Exception as e:\n",
    "        return handle_exception('Unable to load Network Controller from this workspace. {}'.format(str(e)))\n",
    "\n",
    "    data_repo.set_deep_model_info(key='network_controller', data=network_controller)\n",
    "    \n",
    "    return 'success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27e2bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_network_controller():\n",
    "    return data_repo.get_deep_model_info(key='network_controller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0319ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/get_available_networks\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def get_available_networks():\n",
    "    \"\"\"\n",
    "    Access workspace on data repo, set up network controller and reads available nets\n",
    "    find workspace > creates a network controller, reads available networks and its custom parametes\n",
    "    \n",
    "    return available networks\n",
    "    \"\"\"\n",
    "    network_controller = _get_network_controller()\n",
    "    \n",
    "    if not network_controller:\n",
    "        return handle_exception('Unable toload Network Controller. Make sure workspace is loaded.')\n",
    "    \n",
    "    return jsonify(network_controller.network_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900a2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/get_network_params\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def get_network_params(network_name):\n",
    "    # {k: self._custom_params_ui[k].currentText() for k in self._custom_params_ui}\n",
    "    # params = self._network_controller.network_list_params(network)\n",
    "    \n",
    "    network_controller = _get_network_controller()\n",
    "    \n",
    "    if not network_controller:\n",
    "        return handle_exception('Unable toload Network Controller. Make sure workspace is loaded.')\n",
    "    \n",
    "    return jsonify(network_controller.network_list_params(network_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1c5865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/import_network\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def import_network():\n",
    "    import_network_path = request.json['import_network_path']\n",
    "    import_network_name = request.json['import_network_name']\n",
    "\n",
    "    network_controller = _get_network_controller()\n",
    "    \n",
    "    if not network_controller:\n",
    "        return handle_exception('Unable toload Network Controller. Make sure workspace is loaded.')\n",
    "\n",
    "    try:\n",
    "        network_controller.import_model(import_network_path, import_network_name)\n",
    "    except Exception as e:\n",
    "        return handle_exception('Unable to import network. {}'.format(str(e)))\n",
    "\n",
    "    return get_network_params(import_network_name) # make this call on sfetch().then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4d20372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/import_dataset\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def import_dataset():\n",
    "    dataset_fpath = request.json['import_dataset_path']\n",
    "\n",
    "    dataset = dataset.load_dataset(dataset_fpath)\n",
    "    \n",
    "    logging.debug('RUN THREAD ... ')\n",
    "    logging.debug('{}'.format(dataset))\n",
    "\n",
    "    if dataset is None:\n",
    "        return \"Fail. Dataset not found\"\n",
    "\n",
    "    data = dataset['data']\n",
    "    logging.debug('Compute dataset info')\n",
    "\n",
    "    stats = dataset.get_stats(dataset)\n",
    "\n",
    "    # data_info = (data.shape[0], data.shape[1], data.shape[2:], stats['data_mean'], stats['data_std'],\n",
    "    #                    stats['data_min'], stats['data_max'], _dataset['num_classes'], stats['nlabels'],\n",
    "    #                    stats['label_count'], stats['weight_mean'], stats['weight_std'], stats['weight_min'],\n",
    "    #                    stats['weight_max']) #legacy qt code\n",
    "    \n",
    "    data_info_dict = {\n",
    "        '#_images'   : data.shape[0] ,\n",
    "        '#_samples'  : data.shape[1] ,\n",
    "        'Dimensions' : data.shape[2:],\n",
    "        'data_mean': stats['data_mean'],\n",
    "        'data_std' : stats['data_std'],\n",
    "        'data_min' : stats['data_min'],\n",
    "        'data_max' : stats['data_max'],\n",
    "        '#_classes': _dataset['num_classes'], \n",
    "        '#_labels' : stats['nlabels'],\n",
    "        'hist' : stats['label_count'], \n",
    "        'weight_mean' : stats['weight_mean'], \n",
    "        'weight_std' : stats['weight_std'], \n",
    "        'weight_min' : stats['weight_min'],\n",
    "        'weight_max' : stats['weight_max']\n",
    "    }\n",
    "    \n",
    "    data_repo.set_deep_network_info('data_info_dict', data_info_dict)\n",
    "    \n",
    "    logging.debug('Done ...')\n",
    "    return jsonify(data_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667af63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_cache_path():\n",
    "    \n",
    "    cache_path = request.json['cache_path']\n",
    "\n",
    "    network_controller = _get_network_controller()\n",
    "    \n",
    "    if not network_controller:\n",
    "        return handle_exception('Unable toload Network Controller. Make sure workspace is loaded.')\n",
    "    \n",
    "    network_controller.set_cache_base_dir(cache_path)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f35368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_network_custom_parameters(custom_params):\n",
    "#     if not custom_params.keys() in get_network_params().keys()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bd7a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/set_network_training_parameters\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def set_network_training_parameters(parameters):\n",
    "    \n",
    "    _set_network_custom_parameters(parameters['custom_params'])\n",
    "    _set_cache_path(parameters['cache_path'])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a878b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _deploy_new_instance(network_instance_name):\n",
    "    \"\"\"\n",
    "    Returns a new instance object from sscDeepsirius that carries information about the network to be trained.\n",
    "    \n",
    "    Args:\n",
    "        network_instance_name (str): This name is set in instance.network variable\n",
    "    Returns:\n",
    "        instance (Container() struct-like object)\n",
    "    \"\"\"\n",
    "    network_controller = _get_network_controller()\n",
    "    return network_controller.deploy_network_instance(network_instance_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfdf7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_instance(instance_name):\n",
    "    network_controller = _get_network_controller()\n",
    "    return network_controller.get_network_instance(instance_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1afdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _destroy_instance(instance_name):\n",
    "    network_controller = _get_network_controller()\n",
    "    network_controller.destroy_network_instance(instance_name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f39735a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_new_instance(instance_name, network_instance_name):\n",
    "    \"\"\"\n",
    "    Sets new instance of a network, with information to be tracked on the repository and be accessible by the frontend\n",
    "    \n",
    "    instance is an struct like object used in the sscDeepsirius module, with attributes network, train, infer, freeze and finetunr\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    instance = _deploy_new_instance(instance_name)\n",
    "    \n",
    "    instance_ref = {'instance': instance,\n",
    "                    'instance_name': instance_name,\n",
    "                    'network_instance_name': network_instance_name,\n",
    "                    'is_running': False,\n",
    "                    'text_status': '',              \n",
    "                    }\n",
    "    data_repo.set_deep_model_info(key='instance_ref', data=instance_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d37fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_instance_info(instance_info):\n",
    "#     network_controller = _get_network_controller()\n",
    "#     status, message = network_controller.network_instance_status\n",
    "#     self.network_instance_status.setChecked(status)\n",
    "#     if not status:\n",
    "#         message = message + ' (Contact system administrator)'\n",
    "#     self.network_instance_status.setText(message)\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "524dd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/set_tensorboard_server\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def set_tensorboard_server():\n",
    "    network_controller = _get_network_controller()\n",
    "    tensorboard_server_url = network_controller.start_tensorboard(network)\n",
    "    return tensorboard_server_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "352914d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/start_training\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def start_training():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3378abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/start_finetune\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def start_finetune():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32a6f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/export_network\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def export_network():\n",
    "    \"\"\"\n",
    "        path should have extension '.model.tar.gz'\n",
    "    \"\"\"\n",
    "    network_name = request.json['export_network_name']\n",
    "    export_network_path = request.json['export_network_path'] # with new name\n",
    "    \n",
    "    network_controller = _get_network_controller()\n",
    "    \n",
    "    if not network_controller:\n",
    "        return handle_exception('Unable toload Network Controller. Make sure workspace is loaded.')\n",
    "    \n",
    "    if network_name not in network_controller.network_models:\n",
    "        return handle_exception('Network name not in network models list. {}'.format(str(e)))\n",
    "    \n",
    "    # exporting model\n",
    "    try:\n",
    "        network_controller.export_model(network_name, export_network_path)\n",
    "    except Exception as e:\n",
    "        return handle_exception('Unable to export network. {} {}'.format(str(e), export_network_path))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "593316b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/deep/network/export_inference\", methods=[\"POST\"])\n",
    "# @cross_origin()\n",
    "def export_inference():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2840a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# host mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7f4fc",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55724997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting data that would come as input from the @app ['POST'] routes\n",
    "request.set_json({'workspace_path': '/home/brunocarlos_lnls/work/anot/temp/web/workspace',\n",
    " 'dataset_path': '/home/brunocarlos_lnls/work/images/datasets/dataset_augmented.h5',\n",
    " 'cache_path': '/home/brunocarlos_lnls/work/anot/temp/web/workspace/tmp',\n",
    " 'import_network_name':'mynewvnet',\n",
    " 'import_network_path':'/home/brunocarlos_lnls/work/anot/temp/legacy/frozen/my_frozen_vnet.model.tar.gz',\n",
    " 'export_network_name':'unet2d',\n",
    " 'export_network_path':'/home/brunocarlos_lnls/work/anot/temp/web/workspace/frozen/any_name.model.tar.gz'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16b0a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we have to load the workspace in data_repo, which is done in other module of the frontend\n",
    "# setting workspace\n",
    "\n",
    "load_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff38ef4",
   "metadata": {},
   "source": [
    "looking if the workspace path is written on data_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "754d0ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_set_network_controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31512dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newvnet', 'unet3d', 'vnet', 'mynewvnet', 'unet2d']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avail_nets = get_available_networks()\n",
    "avail_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f0c8489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Patch Size': {'type': 'Group',\n",
       "  'value': {'xy': {'type': 'Combo',\n",
       "    'argname': 'patch_size__xy',\n",
       "    'value': ['32,32', '64,64', '128,128', '256,256', '512,512']},\n",
       "   'z': {'type': 'Combo',\n",
       "    'argname': 'patch_size__z',\n",
       "    'value': ['16', '32', '64', '128', '256', '512']}}},\n",
       " 'Drop Classifier': {'type': 'Combo',\n",
       "  'argname': 'drop_classifier',\n",
       "  'value': ['No', 'Yes']}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_network_params(avail_nets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9c93188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Patch Size': {'type': 'Group',\n",
       "  'value': {'xy': {'type': 'Combo',\n",
       "    'argname': 'patch_size__xy',\n",
       "    'value': ['32,32', '64,64', '128,128', '256,256', '512,512']},\n",
       "   'z': {'type': 'Combo',\n",
       "    'argname': 'patch_size__z',\n",
       "    'value': ['16', '32', '64', '128', '256', '512']}}},\n",
       " 'Drop Classifier': {'type': 'Combo',\n",
       "  'argname': 'drop_classifier',\n",
       "  'value': ['No', 'Yes']}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_network() # reads from request.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d17c5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Prune things away ...\n",
      "/home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/models/checkpoint\n",
      "No checkpoint found\n",
      "folder:  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d\n",
      "exported files:  ['.dockerignore', 'docker_connect', 'Dockerfile.ppc64le', 'params.json', 'docker_destroy', 'Dockerfile.x86_64', 'config.cfg', 'Dockerfile', 'default.cfg', 'docker_create', 'train/label/.gitkeep', 'train/data/.gitkeep', 'label/.gitkeep', 'utils/__init__.py', 'utils/sampling.py', 'utils/image.py', 'utils/info.py', 'infer/label/.gitkeep', 'infer/data/.gitkeep', 'bin/train', 'bin/version', 'bin/copy_train', 'bin/infer', 'bin/freeze', 'bin/track', 'bin/finetune', 'bin/install', 'niftynet/settings_training.txt', 'niftynet/label.csv', 'niftynet/dataset_split.csv', 'niftynet/train/.gitkeep', 'niftynet/extensions/config.ini.template', 'niftynet/extensions/__init__.py', 'niftynet/extensions/log.txt', 'niftynet/extensions/engine/__init__.py', 'niftynet/extensions/engine/custom_optimiser.py', 'niftynet/extensions/engine/custom_optimizer.py', 'niftynet/extensions/network/__init__.py']\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/.dockerignore .dockerignore\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/docker_connect docker_connect\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/Dockerfile.ppc64le Dockerfile.ppc64le\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/params.json params.json\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/docker_destroy docker_destroy\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/Dockerfile.x86_64 Dockerfile.x86_64\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/config.cfg config.cfg\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/Dockerfile Dockerfile\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/default.cfg default.cfg\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/docker_create docker_create\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/train/label/.gitkeep train/label/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/train/data/.gitkeep train/data/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/label/.gitkeep label/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/utils/__init__.py utils/__init__.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/utils/sampling.py utils/sampling.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/utils/image.py utils/image.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/utils/info.py utils/info.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/infer/label/.gitkeep infer/label/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/infer/data/.gitkeep infer/data/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/train bin/train\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/version bin/version\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/copy_train bin/copy_train\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/infer bin/infer\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/freeze bin/freeze\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/track bin/track\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/finetune bin/finetune\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/bin/install bin/install\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/settings_training.txt niftynet/settings_training.txt\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/label.csv niftynet/label.csv\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/dataset_split.csv niftynet/dataset_split.csv\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/train/.gitkeep niftynet/train/.gitkeep\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/config.ini.template niftynet/extensions/config.ini.template\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/__init__.py niftynet/extensions/__init__.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/log.txt niftynet/extensions/log.txt\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/engine/__init__.py niftynet/extensions/engine/__init__.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/engine/custom_optimiser.py niftynet/extensions/engine/custom_optimiser.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/engine/custom_optimizer.py niftynet/extensions/engine/custom_optimizer.py\n",
      ">>  /home/brunocarlos_lnls/work/anot/temp/web/workspace/networks/unet2d/niftynet/extensions/network/__init__.py niftynet/extensions/network/__init__.py\n"
     ]
    }
   ],
   "source": [
    "export_network() # export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39470076-83dd-4c52-ac49-de7d789739c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1fd38f56a45b4af55475ed01d2e420e5da8cc6f5d5f3c61fa71b248bde1c6e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
